{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb48bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos,neg),axis = 0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]),axis = 0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices,:]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=10000, random_state=42) \n",
    "\n",
    "# Step 4: Perform five-fold cross-validation and get predictions for validation set\n",
    "predictions_val = cross_val_predict(rf_model, X_train, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Step 5: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Now, fit the model on the full training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Get predictions for the test set\n",
    "predictions_test = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 7: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 8: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05936c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 4: Create the Naive Bayes model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Get predictions for the validation set\n",
    "predictions_val = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Step 7: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Step 8: Get predictions for the test set\n",
    "predictions_test = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 10: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler  # Add this import for scaling the data\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create the SVM model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Step 5: Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n",
    "}\n",
    "\n",
    "# Step 6: Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 7: Get the best model from the GridSearchCV\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 8: Get predictions for the validation set using the best model\n",
    "predictions_val = best_svm_model.decision_function(X_train_scaled)\n",
    "\n",
    "# Step 9: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0).astype(int))\n",
    "\n",
    "# Step 10: Get predictions for the test set using the best model\n",
    "predictions_test = best_svm_model.decision_function(X_test_scaled)\n",
    "\n",
    "# Step 11: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0).astype(int))\n",
    "\n",
    "# Step 12: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regresssion\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler  # Add this import for scaling the data\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create the Logistic Regression model and set max_iter\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 5: Perform five-fold cross-validation and get predictions for validation set\n",
    "predictions_val = cross_val_predict(lr_model, X_train_scaled, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Step 6: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Now, fit the model on the full training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Get predictions for the test set\n",
    "predictions_test = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 8: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 9: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Step 4: Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "    'max_depth': [3, 5, 7],  # Maximum depth of the trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "}\n",
    "\n",
    "# Step 5: Perform hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Get the best model from the GridSearchCV\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 7: Get predictions for the validation set using the best model\n",
    "predictions_val = best_xgb_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Step 8: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Step 9: Get predictions for the test set using the best model\n",
    "predictions_test = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 10: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 11: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost optimized\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.01,\n",
    "    gamma=0.1,\n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric=[\"auc\", \"logloss\"],\n",
    "    early_stopping_rounds=1\n",
    ")\n",
    "\n",
    "# Step 5: Train the model\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Step 6: Get predictions for the validation set\n",
    "predictions_val = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Step 7: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Step 8: Get predictions for the test set\n",
    "predictions_test = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 10: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c47bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_OH_ND.csv') \n",
    "neg = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_OH_ND.csv')\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(1)  # Add a channel dimension for Conv1d\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).unsqueeze(1)  # Add a channel dimension for Conv1d\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Step 5: Create the neural network class\n",
    "class Net_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_conv, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.2)  # 添加第一个Dropout层\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout2 = nn.Dropout(0.2)  # 添加第二个Dropout层\n",
    "        self.fc1 = nn.Linear(6592, 64)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # 添加第三个Dropout层\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.dropout1(x)  # 在第一个卷积层后应用第一个Dropout层\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout2(x)  # 在第二个卷积层后应用第二个Dropout层\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x)) \n",
    "        x = self.dropout3(x)  # 在第一个全连接层后应用第三个Dropout层\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Step 6: Create the model and set the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "model = Net_conv().to(device)\n",
    "\n",
    "# Step 7: Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.93)\n",
    "\n",
    "# Step 8: Train the model\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        inputs = X_train_tensor[i:i+batch_size].to(device)\n",
    "        labels = y_train_tensor[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / (len(X_train_tensor) / batch_size)}\")\n",
    "\n",
    "# Step 9: Evaluate the model on the validation and test sets\n",
    "model.eval()\n",
    "\n",
    "# Validation set\n",
    "with torch.no_grad():\n",
    "    predictions_val = model(X_train_tensor.to(device))\n",
    "    predictions_val = predictions_val.cpu().squeeze().numpy()\n",
    "\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Test set\n",
    "with torch.no_grad():\n",
    "    predictions_test = model(X_test_tensor.to(device))\n",
    "    predictions_test = predictions_test.cpu().squeeze().numpy()\n",
    "\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 10: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/Users/jiaming/Desktop/revision/datas/seq_plus_geo_model_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407882a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52825884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9562b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
