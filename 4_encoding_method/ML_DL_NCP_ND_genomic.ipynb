{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb48bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "pos1 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_NCP_ND.csv') \n",
    "neg1 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_NCP_ND.csv')\n",
    "\n",
    "pos2 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_domain_encoding.csv') \n",
    "neg2 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_domain_encoding.csv')\n",
    "\n",
    "pos = pd.concat([pos1,pos2],axis = 1)\n",
    "neg = pd.concat([neg1,neg2],axis = 1)\n",
    "\n",
    "pos = pos.dropna()\n",
    "neg = neg.dropna()\n",
    "\n",
    "raw_datas = np.concatenate((pos,neg),axis = 0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]),axis = 0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices,:]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=1, random_state=42)\n",
    "\n",
    "# Step 4: Perform five-fold cross-validation and get predictions for validation set\n",
    "predictions_val = cross_val_predict(rf_model, X_train, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Step 5: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Now, fit the model on the full training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Get predictions for the test set\n",
    "predictions_test = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 7: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 8: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos1 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_NCP_ND.csv') \n",
    "neg1 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_NCP_ND.csv')\n",
    "\n",
    "pos2 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_domain_encoding.csv') \n",
    "neg2 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_domain_encoding.csv')\n",
    "\n",
    "pos = pd.concat([pos1,pos2],axis = 1)\n",
    "neg = pd.concat([neg1,neg2],axis = 1)\n",
    "\n",
    "pos = pos.dropna()\n",
    "neg = neg.dropna()\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create the Naive Bayes model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Get predictions for the validation set\n",
    "predictions_val = model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Step 7: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Step 8: Get predictions for the test set\n",
    "predictions_test = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 10: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regresssion\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler  # Add this import for scaling the data\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos1 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_NCP_ND.csv') \n",
    "neg1 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_NCP_ND.csv')\n",
    "\n",
    "pos2 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_domain_encoding.csv') \n",
    "neg2 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_domain_encoding.csv')\n",
    "\n",
    "pos = pd.concat([pos1,pos2],axis = 1)\n",
    "neg = pd.concat([neg1,neg2],axis = 1)\n",
    "\n",
    "pos = pos.dropna()\n",
    "neg = neg.dropna()\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create the Logistic Regression model and set max_iter\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 5: Perform five-fold cross-validation and get predictions for validation set\n",
    "predictions_val = cross_val_predict(lr_model, X_train_scaled, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Step 6: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Now, fit the model on the full training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Get predictions for the test set\n",
    "predictions_test = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 8: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 9: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43946bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost optimized\n",
    "# Step 1: Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Step 2: Load your data and split it into features (X) and labels (y)\n",
    "\n",
    "pos1 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_encoding_NCP_ND.csv') \n",
    "neg1 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_encoding_NCP_ND.csv')\n",
    "\n",
    "pos2 = pd.read_csv('/Users/jiaming/Desktop/f5c/pos_domain_encoding.csv') \n",
    "neg2 = pd.read_csv('/Users/jiaming/Desktop/f5c/neg_domain_encoding.csv')\n",
    "\n",
    "pos = pd.concat([pos1,pos2],axis = 1)\n",
    "neg = pd.concat([neg1,neg2],axis = 1)\n",
    "\n",
    "pos = pos.dropna()\n",
    "neg = neg.dropna()\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)\n",
    "\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(raw_labels.shape[0])\n",
    "\n",
    "\n",
    "X = raw_datas[indices, :]\n",
    "y = raw_labels[indices]\n",
    "\n",
    "# Replace X_train, y_train, X_test, y_test with your actual data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=1,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.0001,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=10.0,      # L1 regularization\n",
    "    reg_lambda=100.0,     # L2 regularization\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.2,\n",
    "    random_state=42,\n",
    "    eval_metric=[\"auc\", \"logloss\"],\n",
    "    early_stopping_rounds=1\n",
    ")\n",
    "\n",
    "# Step 5: Train the model\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 6: Get predictions for the validation set\n",
    "predictions_val = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Step 7: Calculate evaluation metrics for the validation set\n",
    "AUC_val = roc_auc_score(y_train, predictions_val)\n",
    "ACC_val = accuracy_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "MCC_val = matthews_corrcoef(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sn_val = recall_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "Sp_val = precision_score(y_train, (predictions_val > 0.5).astype(int))\n",
    "\n",
    "# Step 8: Get predictions for the test set\n",
    "predictions_test = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Step 9: Calculate evaluation metrics for the test set\n",
    "AUC_test = roc_auc_score(y_test, predictions_test)\n",
    "ACC_test = accuracy_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "MCC_test = matthews_corrcoef(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sn_test = recall_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "Sp_test = precision_score(y_test, (predictions_test > 0.5).astype(int))\n",
    "\n",
    "# Step 10: Print the results\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "print(\"Sp_test: {:.4f}\".format(Sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5708a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183ea77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407882a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52825884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9562b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
