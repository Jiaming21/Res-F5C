{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06a1d40",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d24dfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e883f",
   "metadata": {},
   "source": [
    "# Settings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75faf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 123\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0d17de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table = pd.read_csv('/Users/jiaming/Desktop/revision/datas/pos_encoding_OH_ND.csv')\n",
    "neg_table = pd.read_csv('/Users/jiaming/Desktop/revision/datas/neg_encoding_OH_ND.csv')\n",
    "\n",
    "pos_table = pos_table.iloc[:,1:]\n",
    "neg_table = neg_table.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b15f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_geo = pd.read_csv('/Users/jiaming/Desktop/revision/datas/pos_domain_encoding.csv')\n",
    "neg_geo = pd.read_csv('/Users/jiaming/Desktop/revision/datas/neg_domain_encoding.csv')\n",
    "\n",
    "pos_geo = pos_geo.iloc[:,1:]\n",
    "neg_geo = neg_geo.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccd46ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1892, 1, 245])\n",
      "torch.Size([18920, 1, 245])\n"
     ]
    }
   ],
   "source": [
    "pos_cb = pd.concat([pos_table, pos_geo], axis=1)\n",
    "neg_cb = pd.concat([neg_table, neg_geo], axis=1)\n",
    "\n",
    "pos_np = pos_cb.to_numpy()\n",
    "neg_np = neg_cb.to_numpy()\n",
    "\n",
    "pos = torch.FloatTensor(pos_np).unsqueeze(1)\n",
    "neg = torch.FloatTensor(neg_np).unsqueeze(1)\n",
    "\n",
    "print(pos.shape)\n",
    "print(neg.shape)\n",
    "\n",
    "raw_datas = np.concatenate((pos, neg), axis=0)\n",
    "raw_labels = np.concatenate(([1] * pos.shape[0], [0] * neg.shape[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53c17dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(raw_datas, raw_labels, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a7b8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[np.isnan(data_train)] = 0\n",
    "data_test[np.isnan(data_test)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b8f627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "train_dataset_unsplit = CustomDataset(data_train, labels_train)\n",
    "test_dataset = CustomDataset(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9648d4a",
   "metadata": {},
   "source": [
    "# Kfold for train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "451aa890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# \n",
    "# kf = KFold(n_splits=5)\n",
    "# \n",
    "# i = 1\n",
    "# for train_dataset, val_dataset in kf.split(X=train_dataset):\n",
    "#     print(\"iteration \", i)\n",
    "#     print(train_dataset, \" having :\" , len(train_dataset))\n",
    "#     print(val_dataset, \" having :\" , len(val_dataset))\n",
    "#     print(\"-------------------------\")\n",
    "#     i += 1\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5aa061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(dataset=train_dataset, \n",
    "#                           batch_size=batch_size, \n",
    "#                           shuffle=True)\n",
    "# \n",
    "# test_loader = DataLoader(dataset=test_dataset, \n",
    "#                          batch_size=batch_size, \n",
    "#                          shuffle=False)\n",
    "# \n",
    "# # Checking the dataset \n",
    "# for images, labels in train_loader: # 随意load一次, 可无限循环, 必须break\n",
    "#     print('Image batch dimensions:', images.shape)\n",
    "#     print('Image label dimensions:', labels.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66377b",
   "metadata": {},
   "source": [
    "# ResNet with identity blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3125fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        #########################\n",
    "        ### 1st residual block\n",
    "        #########################\n",
    "        \n",
    "        self.block_1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=1,\n",
    "                                out_channels=4,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=0),\n",
    "                torch.nn.BatchNorm1d(4),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv1d(in_channels=4,\n",
    "                                out_channels=1,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm1d(1)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=1,\n",
    "                                out_channels=4,\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=0),\n",
    "                torch.nn.BatchNorm1d(4),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Conv1d(in_channels=4,\n",
    "                                out_channels=1,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm1d(1)\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        ### Fully connected\n",
    "        #########################        \n",
    "        self.linear_1 = torch.nn.Linear(245, num_classes) # 205 = 26240/batch_size\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #########################\n",
    "        ### 1st residual block\n",
    "        #########################\n",
    "        shortcut = x\n",
    "        x = self.block_1(x)\n",
    "        x = torch.nn.functional.relu(x + shortcut)\n",
    "        \n",
    "        #########################\n",
    "        ### 2nd residual block\n",
    "        #########################\n",
    "        shortcut = x\n",
    "        x = self.block_2(x)\n",
    "        x = torch.nn.functional.relu(x + shortcut)\n",
    "        \n",
    "        #########################\n",
    "        ### Fully connected\n",
    "        #########################\n",
    "        logits = self.linear_1(x.view(-1, 245))\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7dd7d0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "007a9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(features)\n",
    "        _, predicted_labels = torch.max(logits, dim=1) \n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f7c6b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "train_dataset_indx  having : 13319\n",
      "val_dataset_indx  having : 3330\n",
      "-------------------------\n",
      "Image batch dimensions: torch.Size([128, 1, 245])\n",
      "Image label dimensions: torch.Size([128])\n",
      "iteration  2\n",
      "train_dataset_indx  having : 13319\n",
      "val_dataset_indx  having : 3330\n",
      "-------------------------\n",
      "Image batch dimensions: torch.Size([128, 1, 245])\n",
      "Image label dimensions: torch.Size([128])\n",
      "iteration  3\n",
      "train_dataset_indx  having : 13319\n",
      "val_dataset_indx  having : 3330\n",
      "-------------------------\n",
      "Image batch dimensions: torch.Size([128, 1, 245])\n",
      "Image label dimensions: torch.Size([128])\n",
      "iteration  4\n",
      "train_dataset_indx  having : 13319\n",
      "val_dataset_indx  having : 3330\n",
      "-------------------------\n",
      "Image batch dimensions: torch.Size([128, 1, 245])\n",
      "Image label dimensions: torch.Size([128])\n",
      "iteration  5\n",
      "train_dataset_indx  having : 13320\n",
      "val_dataset_indx  having : 3329\n",
      "-------------------------\n",
      "Image batch dimensions: torch.Size([128, 1, 245])\n",
      "Image label dimensions: torch.Size([128])\n",
      "Epoch: 001/050 | Batch 000/105 | Cost: 0.5788\n",
      "Epoch: 001/050 | Batch 010/105 | Cost: 0.6066\n",
      "Epoch: 001/050 | Batch 020/105 | Cost: 0.6816\n",
      "Epoch: 001/050 | Batch 030/105 | Cost: 0.6814\n",
      "Epoch: 001/050 | Batch 040/105 | Cost: 0.6749\n",
      "Epoch: 001/050 | Batch 050/105 | Cost: 0.6576\n",
      "Epoch: 001/050 | Batch 060/105 | Cost: 0.5975\n",
      "Epoch: 001/050 | Batch 070/105 | Cost: 0.6216\n",
      "Epoch: 001/050 | Batch 080/105 | Cost: 0.6389\n",
      "Epoch: 001/050 | Batch 090/105 | Cost: 0.5903\n",
      "Epoch: 001/050 | Batch 100/105 | Cost: 0.5788\n",
      "Epoch: 001/050 training accuracy: 87.70%\n",
      "Epoch: 002/050 | Batch 000/105 | Cost: 0.6870\n",
      "Epoch: 002/050 | Batch 010/105 | Cost: 0.5930\n",
      "Epoch: 002/050 | Batch 020/105 | Cost: 0.5882\n",
      "Epoch: 002/050 | Batch 030/105 | Cost: 0.5673\n",
      "Epoch: 002/050 | Batch 040/105 | Cost: 0.5517\n",
      "Epoch: 002/050 | Batch 050/105 | Cost: 0.5427\n",
      "Epoch: 002/050 | Batch 060/105 | Cost: 0.5763\n",
      "Epoch: 002/050 | Batch 070/105 | Cost: 0.6089\n",
      "Epoch: 002/050 | Batch 080/105 | Cost: 0.5834\n",
      "Epoch: 002/050 | Batch 090/105 | Cost: 0.5762\n",
      "Epoch: 002/050 | Batch 100/105 | Cost: 0.5765\n",
      "Epoch: 002/050 training accuracy: 89.29%\n",
      "Epoch: 003/050 | Batch 000/105 | Cost: 0.5818\n",
      "Epoch: 003/050 | Batch 010/105 | Cost: 0.5772\n",
      "Epoch: 003/050 | Batch 020/105 | Cost: 0.5250\n",
      "Epoch: 003/050 | Batch 030/105 | Cost: 0.5590\n",
      "Epoch: 003/050 | Batch 040/105 | Cost: 0.5242\n",
      "Epoch: 003/050 | Batch 050/105 | Cost: 0.5436\n",
      "Epoch: 003/050 | Batch 060/105 | Cost: 0.4852\n",
      "Epoch: 003/050 | Batch 070/105 | Cost: 0.5566\n",
      "Epoch: 003/050 | Batch 080/105 | Cost: 0.5299\n",
      "Epoch: 003/050 | Batch 090/105 | Cost: 0.5535\n",
      "Epoch: 003/050 | Batch 100/105 | Cost: 0.4784\n",
      "Epoch: 003/050 training accuracy: 90.05%\n",
      "Epoch: 004/050 | Batch 000/105 | Cost: 0.5828\n",
      "Epoch: 004/050 | Batch 010/105 | Cost: 0.4885\n",
      "Epoch: 004/050 | Batch 020/105 | Cost: 0.5575\n",
      "Epoch: 004/050 | Batch 030/105 | Cost: 0.4720\n",
      "Epoch: 004/050 | Batch 040/105 | Cost: 0.4566\n",
      "Epoch: 004/050 | Batch 050/105 | Cost: 0.4759\n",
      "Epoch: 004/050 | Batch 060/105 | Cost: 0.5534\n",
      "Epoch: 004/050 | Batch 070/105 | Cost: 0.4545\n",
      "Epoch: 004/050 | Batch 080/105 | Cost: 0.4775\n",
      "Epoch: 004/050 | Batch 090/105 | Cost: 0.5067\n",
      "Epoch: 004/050 | Batch 100/105 | Cost: 0.4296\n",
      "Epoch: 004/050 training accuracy: 90.43%\n",
      "Epoch: 005/050 | Batch 000/105 | Cost: 0.5255\n",
      "Epoch: 005/050 | Batch 010/105 | Cost: 0.4357\n",
      "Epoch: 005/050 | Batch 020/105 | Cost: 0.4722\n",
      "Epoch: 005/050 | Batch 030/105 | Cost: 0.4880\n",
      "Epoch: 005/050 | Batch 040/105 | Cost: 0.4986\n",
      "Epoch: 005/050 | Batch 050/105 | Cost: 0.5175\n",
      "Epoch: 005/050 | Batch 060/105 | Cost: 0.5193\n",
      "Epoch: 005/050 | Batch 070/105 | Cost: 0.5058\n",
      "Epoch: 005/050 | Batch 080/105 | Cost: 0.5300\n",
      "Epoch: 005/050 | Batch 090/105 | Cost: 0.4847\n",
      "Epoch: 005/050 | Batch 100/105 | Cost: 0.4731\n",
      "Epoch: 005/050 training accuracy: 90.53%\n",
      "Epoch: 006/050 | Batch 000/105 | Cost: 0.4719\n",
      "Epoch: 006/050 | Batch 010/105 | Cost: 0.5385\n",
      "Epoch: 006/050 | Batch 020/105 | Cost: 0.3939\n",
      "Epoch: 006/050 | Batch 030/105 | Cost: 0.4635\n",
      "Epoch: 006/050 | Batch 040/105 | Cost: 0.4972\n",
      "Epoch: 006/050 | Batch 050/105 | Cost: 0.4521\n",
      "Epoch: 006/050 | Batch 060/105 | Cost: 0.4494\n",
      "Epoch: 006/050 | Batch 070/105 | Cost: 0.4317\n",
      "Epoch: 006/050 | Batch 080/105 | Cost: 0.4658\n",
      "Epoch: 006/050 | Batch 090/105 | Cost: 0.3927\n",
      "Epoch: 006/050 | Batch 100/105 | Cost: 0.4281\n",
      "Epoch: 006/050 training accuracy: 90.54%\n",
      "Epoch: 007/050 | Batch 000/105 | Cost: 0.5131\n",
      "Epoch: 007/050 | Batch 010/105 | Cost: 0.4112\n",
      "Epoch: 007/050 | Batch 020/105 | Cost: 0.4593\n",
      "Epoch: 007/050 | Batch 030/105 | Cost: 0.3893\n",
      "Epoch: 007/050 | Batch 040/105 | Cost: 0.4099\n",
      "Epoch: 007/050 | Batch 050/105 | Cost: 0.3862\n",
      "Epoch: 007/050 | Batch 060/105 | Cost: 0.4647\n",
      "Epoch: 007/050 | Batch 070/105 | Cost: 0.4008\n",
      "Epoch: 007/050 | Batch 080/105 | Cost: 0.4156\n",
      "Epoch: 007/050 | Batch 090/105 | Cost: 0.4295\n",
      "Epoch: 007/050 | Batch 100/105 | Cost: 0.4570\n",
      "Epoch: 007/050 training accuracy: 90.54%\n",
      "Epoch: 008/050 | Batch 000/105 | Cost: 0.4150\n",
      "Epoch: 008/050 | Batch 010/105 | Cost: 0.3845\n",
      "Epoch: 008/050 | Batch 020/105 | Cost: 0.4634\n",
      "Epoch: 008/050 | Batch 030/105 | Cost: 0.3968\n",
      "Epoch: 008/050 | Batch 040/105 | Cost: 0.4212\n",
      "Epoch: 008/050 | Batch 050/105 | Cost: 0.4095\n",
      "Epoch: 008/050 | Batch 060/105 | Cost: 0.3891\n",
      "Epoch: 008/050 | Batch 070/105 | Cost: 0.4060\n",
      "Epoch: 008/050 | Batch 080/105 | Cost: 0.4079\n",
      "Epoch: 008/050 | Batch 090/105 | Cost: 0.4450\n",
      "Epoch: 008/050 | Batch 100/105 | Cost: 0.3776\n",
      "Epoch: 008/050 training accuracy: 90.64%\n",
      "Epoch: 009/050 | Batch 000/105 | Cost: 0.3690\n",
      "Epoch: 009/050 | Batch 010/105 | Cost: 0.4481\n",
      "Epoch: 009/050 | Batch 020/105 | Cost: 0.4146\n",
      "Epoch: 009/050 | Batch 030/105 | Cost: 0.3885\n",
      "Epoch: 009/050 | Batch 040/105 | Cost: 0.3980\n",
      "Epoch: 009/050 | Batch 050/105 | Cost: 0.4578\n",
      "Epoch: 009/050 | Batch 060/105 | Cost: 0.3567\n",
      "Epoch: 009/050 | Batch 070/105 | Cost: 0.3708\n",
      "Epoch: 009/050 | Batch 080/105 | Cost: 0.4581\n",
      "Epoch: 009/050 | Batch 090/105 | Cost: 0.3586\n",
      "Epoch: 009/050 | Batch 100/105 | Cost: 0.3790\n",
      "Epoch: 009/050 training accuracy: 90.64%\n",
      "Epoch: 010/050 | Batch 000/105 | Cost: 0.4045\n",
      "Epoch: 010/050 | Batch 010/105 | Cost: 0.4659\n",
      "Epoch: 010/050 | Batch 020/105 | Cost: 0.3972\n",
      "Epoch: 010/050 | Batch 030/105 | Cost: 0.3283\n",
      "Epoch: 010/050 | Batch 040/105 | Cost: 0.3669\n",
      "Epoch: 010/050 | Batch 050/105 | Cost: 0.3452\n",
      "Epoch: 010/050 | Batch 060/105 | Cost: 0.4405\n",
      "Epoch: 010/050 | Batch 070/105 | Cost: 0.3427\n",
      "Epoch: 010/050 | Batch 080/105 | Cost: 0.3440\n",
      "Epoch: 010/050 | Batch 090/105 | Cost: 0.3597\n",
      "Epoch: 010/050 | Batch 100/105 | Cost: 0.3317\n",
      "Epoch: 010/050 training accuracy: 90.72%\n",
      "Epoch: 011/050 | Batch 000/105 | Cost: 0.3809\n",
      "Epoch: 011/050 | Batch 010/105 | Cost: 0.3633\n",
      "Epoch: 011/050 | Batch 020/105 | Cost: 0.2780\n",
      "Epoch: 011/050 | Batch 030/105 | Cost: 0.3511\n",
      "Epoch: 011/050 | Batch 040/105 | Cost: 0.3229\n",
      "Epoch: 011/050 | Batch 050/105 | Cost: 0.3775\n",
      "Epoch: 011/050 | Batch 060/105 | Cost: 0.3485\n",
      "Epoch: 011/050 | Batch 070/105 | Cost: 0.3574\n",
      "Epoch: 011/050 | Batch 080/105 | Cost: 0.3428\n",
      "Epoch: 011/050 | Batch 090/105 | Cost: 0.3275\n",
      "Epoch: 011/050 | Batch 100/105 | Cost: 0.3336\n",
      "Epoch: 011/050 training accuracy: 90.71%\n",
      "Epoch: 012/050 | Batch 000/105 | Cost: 0.4130\n",
      "Epoch: 012/050 | Batch 010/105 | Cost: 0.2793\n",
      "Epoch: 012/050 | Batch 020/105 | Cost: 0.4178\n",
      "Epoch: 012/050 | Batch 030/105 | Cost: 0.3280\n",
      "Epoch: 012/050 | Batch 040/105 | Cost: 0.3182\n",
      "Epoch: 012/050 | Batch 050/105 | Cost: 0.3518\n",
      "Epoch: 012/050 | Batch 060/105 | Cost: 0.3576\n",
      "Epoch: 012/050 | Batch 070/105 | Cost: 0.3586\n",
      "Epoch: 012/050 | Batch 080/105 | Cost: 0.3398\n",
      "Epoch: 012/050 | Batch 090/105 | Cost: 0.3193\n",
      "Epoch: 012/050 | Batch 100/105 | Cost: 0.2618\n",
      "Epoch: 012/050 training accuracy: 90.77%\n",
      "Epoch: 013/050 | Batch 000/105 | Cost: 0.3381\n",
      "Epoch: 013/050 | Batch 010/105 | Cost: 0.3430\n",
      "Epoch: 013/050 | Batch 020/105 | Cost: 0.3368\n",
      "Epoch: 013/050 | Batch 030/105 | Cost: 0.3484\n",
      "Epoch: 013/050 | Batch 040/105 | Cost: 0.3766\n",
      "Epoch: 013/050 | Batch 050/105 | Cost: 0.3611\n",
      "Epoch: 013/050 | Batch 060/105 | Cost: 0.3302\n",
      "Epoch: 013/050 | Batch 070/105 | Cost: 0.3549\n",
      "Epoch: 013/050 | Batch 080/105 | Cost: 0.2736\n",
      "Epoch: 013/050 | Batch 090/105 | Cost: 0.3501\n",
      "Epoch: 013/050 | Batch 100/105 | Cost: 0.3490\n",
      "Epoch: 013/050 training accuracy: 90.76%\n",
      "Epoch: 014/050 | Batch 000/105 | Cost: 0.2914\n",
      "Epoch: 014/050 | Batch 010/105 | Cost: 0.3020\n",
      "Epoch: 014/050 | Batch 020/105 | Cost: 0.3009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014/050 | Batch 030/105 | Cost: 0.3638\n",
      "Epoch: 014/050 | Batch 040/105 | Cost: 0.2783\n",
      "Epoch: 014/050 | Batch 050/105 | Cost: 0.3256\n",
      "Epoch: 014/050 | Batch 060/105 | Cost: 0.2963\n",
      "Epoch: 014/050 | Batch 070/105 | Cost: 0.3090\n",
      "Epoch: 014/050 | Batch 080/105 | Cost: 0.3289\n",
      "Epoch: 014/050 | Batch 090/105 | Cost: 0.3047\n",
      "Epoch: 014/050 | Batch 100/105 | Cost: 0.2333\n",
      "Epoch: 014/050 training accuracy: 90.88%\n",
      "Epoch: 015/050 | Batch 000/105 | Cost: 0.2403\n",
      "Epoch: 015/050 | Batch 010/105 | Cost: 0.2854\n",
      "Epoch: 015/050 | Batch 020/105 | Cost: 0.2863\n",
      "Epoch: 015/050 | Batch 030/105 | Cost: 0.2228\n",
      "Epoch: 015/050 | Batch 040/105 | Cost: 0.2963\n",
      "Epoch: 015/050 | Batch 050/105 | Cost: 0.2965\n",
      "Epoch: 015/050 | Batch 060/105 | Cost: 0.3027\n",
      "Epoch: 015/050 | Batch 070/105 | Cost: 0.2940\n",
      "Epoch: 015/050 | Batch 080/105 | Cost: 0.2694\n",
      "Epoch: 015/050 | Batch 090/105 | Cost: 0.3663\n",
      "Epoch: 015/050 | Batch 100/105 | Cost: 0.2652\n",
      "Epoch: 015/050 training accuracy: 90.92%\n",
      "Epoch: 016/050 | Batch 000/105 | Cost: 0.2546\n",
      "Epoch: 016/050 | Batch 010/105 | Cost: 0.2520\n",
      "Epoch: 016/050 | Batch 020/105 | Cost: 0.2914\n",
      "Epoch: 016/050 | Batch 030/105 | Cost: 0.3469\n",
      "Epoch: 016/050 | Batch 040/105 | Cost: 0.2762\n",
      "Epoch: 016/050 | Batch 050/105 | Cost: 0.2476\n",
      "Epoch: 016/050 | Batch 060/105 | Cost: 0.3376\n",
      "Epoch: 016/050 | Batch 070/105 | Cost: 0.2922\n",
      "Epoch: 016/050 | Batch 080/105 | Cost: 0.3077\n",
      "Epoch: 016/050 | Batch 090/105 | Cost: 0.3010\n",
      "Epoch: 016/050 | Batch 100/105 | Cost: 0.3143\n",
      "Epoch: 016/050 training accuracy: 90.92%\n",
      "Epoch: 017/050 | Batch 000/105 | Cost: 0.3000\n",
      "Epoch: 017/050 | Batch 010/105 | Cost: 0.3119\n",
      "Epoch: 017/050 | Batch 020/105 | Cost: 0.2292\n",
      "Epoch: 017/050 | Batch 030/105 | Cost: 0.2745\n",
      "Epoch: 017/050 | Batch 040/105 | Cost: 0.1739\n",
      "Epoch: 017/050 | Batch 050/105 | Cost: 0.2454\n",
      "Epoch: 017/050 | Batch 060/105 | Cost: 0.3320\n",
      "Epoch: 017/050 | Batch 070/105 | Cost: 0.2834\n",
      "Epoch: 017/050 | Batch 080/105 | Cost: 0.2890\n",
      "Epoch: 017/050 | Batch 090/105 | Cost: 0.2753\n",
      "Epoch: 017/050 | Batch 100/105 | Cost: 0.2480\n",
      "Epoch: 017/050 training accuracy: 90.97%\n",
      "Epoch: 018/050 | Batch 000/105 | Cost: 0.2869\n",
      "Epoch: 018/050 | Batch 010/105 | Cost: 0.2944\n",
      "Epoch: 018/050 | Batch 020/105 | Cost: 0.2618\n",
      "Epoch: 018/050 | Batch 030/105 | Cost: 0.2554\n",
      "Epoch: 018/050 | Batch 040/105 | Cost: 0.2220\n",
      "Epoch: 018/050 | Batch 050/105 | Cost: 0.1955\n",
      "Epoch: 018/050 | Batch 060/105 | Cost: 0.2193\n",
      "Epoch: 018/050 | Batch 070/105 | Cost: 0.2059\n",
      "Epoch: 018/050 | Batch 080/105 | Cost: 0.2269\n",
      "Epoch: 018/050 | Batch 090/105 | Cost: 0.2849\n",
      "Epoch: 018/050 | Batch 100/105 | Cost: 0.2191\n",
      "Epoch: 018/050 training accuracy: 90.79%\n",
      "Epoch: 019/050 | Batch 000/105 | Cost: 0.2510\n",
      "Epoch: 019/050 | Batch 010/105 | Cost: 0.2281\n",
      "Epoch: 019/050 | Batch 020/105 | Cost: 0.2359\n",
      "Epoch: 019/050 | Batch 030/105 | Cost: 0.2337\n",
      "Epoch: 019/050 | Batch 040/105 | Cost: 0.1608\n",
      "Epoch: 019/050 | Batch 050/105 | Cost: 0.2598\n",
      "Epoch: 019/050 | Batch 060/105 | Cost: 0.2389\n",
      "Epoch: 019/050 | Batch 070/105 | Cost: 0.2261\n",
      "Epoch: 019/050 | Batch 080/105 | Cost: 0.2465\n",
      "Epoch: 019/050 | Batch 090/105 | Cost: 0.2787\n",
      "Epoch: 019/050 | Batch 100/105 | Cost: 0.2153\n",
      "Epoch: 019/050 training accuracy: 90.71%\n",
      "Epoch: 020/050 | Batch 000/105 | Cost: 0.2399\n",
      "Epoch: 020/050 | Batch 010/105 | Cost: 0.1994\n",
      "Epoch: 020/050 | Batch 020/105 | Cost: 0.1552\n",
      "Epoch: 020/050 | Batch 030/105 | Cost: 0.3380\n",
      "Epoch: 020/050 | Batch 040/105 | Cost: 0.2074\n",
      "Epoch: 020/050 | Batch 050/105 | Cost: 0.2334\n",
      "Epoch: 020/050 | Batch 060/105 | Cost: 0.2288\n",
      "Epoch: 020/050 | Batch 070/105 | Cost: 0.2323\n",
      "Epoch: 020/050 | Batch 080/105 | Cost: 0.2105\n",
      "Epoch: 020/050 | Batch 090/105 | Cost: 0.1947\n",
      "Epoch: 020/050 | Batch 100/105 | Cost: 0.1631\n",
      "Epoch: 020/050 training accuracy: 90.58%\n",
      "Epoch: 021/050 | Batch 000/105 | Cost: 0.1824\n",
      "Epoch: 021/050 | Batch 010/105 | Cost: 0.2479\n",
      "Epoch: 021/050 | Batch 020/105 | Cost: 0.2232\n",
      "Epoch: 021/050 | Batch 030/105 | Cost: 0.2560\n",
      "Epoch: 021/050 | Batch 040/105 | Cost: 0.2334\n",
      "Epoch: 021/050 | Batch 050/105 | Cost: 0.2246\n",
      "Epoch: 021/050 | Batch 060/105 | Cost: 0.2325\n",
      "Epoch: 021/050 | Batch 070/105 | Cost: 0.2037\n",
      "Epoch: 021/050 | Batch 080/105 | Cost: 0.2014\n",
      "Epoch: 021/050 | Batch 090/105 | Cost: 0.2058\n",
      "Epoch: 021/050 | Batch 100/105 | Cost: 0.2502\n",
      "Epoch: 021/050 training accuracy: 90.51%\n",
      "Epoch: 022/050 | Batch 000/105 | Cost: 0.1822\n",
      "Epoch: 022/050 | Batch 010/105 | Cost: 0.2032\n",
      "Epoch: 022/050 | Batch 020/105 | Cost: 0.1826\n",
      "Epoch: 022/050 | Batch 030/105 | Cost: 0.2849\n",
      "Epoch: 022/050 | Batch 040/105 | Cost: 0.1796\n",
      "Epoch: 022/050 | Batch 050/105 | Cost: 0.2148\n",
      "Epoch: 022/050 | Batch 060/105 | Cost: 0.1513\n",
      "Epoch: 022/050 | Batch 070/105 | Cost: 0.1539\n",
      "Epoch: 022/050 | Batch 080/105 | Cost: 0.1830\n",
      "Epoch: 022/050 | Batch 090/105 | Cost: 0.2906\n",
      "Epoch: 022/050 | Batch 100/105 | Cost: 0.3121\n",
      "Epoch: 022/050 training accuracy: 90.43%\n",
      "Epoch: 023/050 | Batch 000/105 | Cost: 0.2813\n",
      "Epoch: 023/050 | Batch 010/105 | Cost: 0.1886\n",
      "Epoch: 023/050 | Batch 020/105 | Cost: 0.1905\n",
      "Epoch: 023/050 | Batch 030/105 | Cost: 0.1531\n",
      "Epoch: 023/050 | Batch 040/105 | Cost: 0.2443\n",
      "Epoch: 023/050 | Batch 050/105 | Cost: 0.1930\n",
      "Epoch: 023/050 | Batch 060/105 | Cost: 0.1817\n",
      "Epoch: 023/050 | Batch 070/105 | Cost: 0.1675\n",
      "Epoch: 023/050 | Batch 080/105 | Cost: 0.2192\n",
      "Epoch: 023/050 | Batch 090/105 | Cost: 0.2017\n",
      "Epoch: 023/050 | Batch 100/105 | Cost: 0.2431\n",
      "Epoch: 023/050 training accuracy: 90.41%\n",
      "Epoch: 024/050 | Batch 000/105 | Cost: 0.2118\n",
      "Epoch: 024/050 | Batch 010/105 | Cost: 0.2464\n",
      "Epoch: 024/050 | Batch 020/105 | Cost: 0.1845\n",
      "Epoch: 024/050 | Batch 030/105 | Cost: 0.2200\n",
      "Epoch: 024/050 | Batch 040/105 | Cost: 0.2673\n",
      "Epoch: 024/050 | Batch 050/105 | Cost: 0.1861\n",
      "Epoch: 024/050 | Batch 060/105 | Cost: 0.2601\n",
      "Epoch: 024/050 | Batch 070/105 | Cost: 0.1633\n",
      "Epoch: 024/050 | Batch 080/105 | Cost: 0.2175\n",
      "Epoch: 024/050 | Batch 090/105 | Cost: 0.2449\n",
      "Epoch: 024/050 | Batch 100/105 | Cost: 0.1842\n",
      "Epoch: 024/050 training accuracy: 90.44%\n",
      "Epoch: 025/050 | Batch 000/105 | Cost: 0.1847\n",
      "Epoch: 025/050 | Batch 010/105 | Cost: 0.1742\n",
      "Epoch: 025/050 | Batch 020/105 | Cost: 0.1747\n",
      "Epoch: 025/050 | Batch 030/105 | Cost: 0.2112\n",
      "Epoch: 025/050 | Batch 040/105 | Cost: 0.2487\n",
      "Epoch: 025/050 | Batch 050/105 | Cost: 0.1920\n",
      "Epoch: 025/050 | Batch 060/105 | Cost: 0.2460\n",
      "Epoch: 025/050 | Batch 070/105 | Cost: 0.2303\n",
      "Epoch: 025/050 | Batch 080/105 | Cost: 0.1786\n",
      "Epoch: 025/050 | Batch 090/105 | Cost: 0.1703\n",
      "Epoch: 025/050 | Batch 100/105 | Cost: 0.1559\n",
      "Epoch: 025/050 training accuracy: 90.44%\n",
      "Epoch: 026/050 | Batch 000/105 | Cost: 0.1693\n",
      "Epoch: 026/050 | Batch 010/105 | Cost: 0.2239\n",
      "Epoch: 026/050 | Batch 020/105 | Cost: 0.2050\n",
      "Epoch: 026/050 | Batch 030/105 | Cost: 0.1912\n",
      "Epoch: 026/050 | Batch 040/105 | Cost: 0.2453\n",
      "Epoch: 026/050 | Batch 050/105 | Cost: 0.2855\n",
      "Epoch: 026/050 | Batch 060/105 | Cost: 0.1421\n",
      "Epoch: 026/050 | Batch 070/105 | Cost: 0.1937\n",
      "Epoch: 026/050 | Batch 080/105 | Cost: 0.2016\n",
      "Epoch: 026/050 | Batch 090/105 | Cost: 0.1569\n",
      "Epoch: 026/050 | Batch 100/105 | Cost: 0.1644\n",
      "Epoch: 026/050 training accuracy: 90.47%\n",
      "Epoch: 027/050 | Batch 000/105 | Cost: 0.1955\n",
      "Epoch: 027/050 | Batch 010/105 | Cost: 0.2608\n",
      "Epoch: 027/050 | Batch 020/105 | Cost: 0.2358\n",
      "Epoch: 027/050 | Batch 030/105 | Cost: 0.2173\n",
      "Epoch: 027/050 | Batch 040/105 | Cost: 0.1832\n",
      "Epoch: 027/050 | Batch 050/105 | Cost: 0.2212\n",
      "Epoch: 027/050 | Batch 060/105 | Cost: 0.1331\n",
      "Epoch: 027/050 | Batch 070/105 | Cost: 0.2195\n",
      "Epoch: 027/050 | Batch 080/105 | Cost: 0.2487\n",
      "Epoch: 027/050 | Batch 090/105 | Cost: 0.2115\n",
      "Epoch: 027/050 | Batch 100/105 | Cost: 0.1826\n",
      "Epoch: 027/050 training accuracy: 90.48%\n",
      "Epoch: 028/050 | Batch 000/105 | Cost: 0.1999\n",
      "Epoch: 028/050 | Batch 010/105 | Cost: 0.2553\n",
      "Epoch: 028/050 | Batch 020/105 | Cost: 0.1937\n",
      "Epoch: 028/050 | Batch 030/105 | Cost: 0.2766\n",
      "Epoch: 028/050 | Batch 040/105 | Cost: 0.1560\n",
      "Epoch: 028/050 | Batch 050/105 | Cost: 0.1914\n",
      "Epoch: 028/050 | Batch 060/105 | Cost: 0.1874\n",
      "Epoch: 028/050 | Batch 070/105 | Cost: 0.2372\n",
      "Epoch: 028/050 | Batch 080/105 | Cost: 0.1350\n",
      "Epoch: 028/050 | Batch 090/105 | Cost: 0.1844\n",
      "Epoch: 028/050 | Batch 100/105 | Cost: 0.2935\n",
      "Epoch: 028/050 training accuracy: 90.45%\n",
      "Epoch: 029/050 | Batch 000/105 | Cost: 0.2770\n",
      "Epoch: 029/050 | Batch 010/105 | Cost: 0.1462\n",
      "Epoch: 029/050 | Batch 020/105 | Cost: 0.1635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029/050 | Batch 030/105 | Cost: 0.1983\n",
      "Epoch: 029/050 | Batch 040/105 | Cost: 0.1283\n",
      "Epoch: 029/050 | Batch 050/105 | Cost: 0.2052\n",
      "Epoch: 029/050 | Batch 060/105 | Cost: 0.1660\n",
      "Epoch: 029/050 | Batch 070/105 | Cost: 0.2170\n",
      "Epoch: 029/050 | Batch 080/105 | Cost: 0.1673\n",
      "Epoch: 029/050 | Batch 090/105 | Cost: 0.2032\n",
      "Epoch: 029/050 | Batch 100/105 | Cost: 0.2314\n",
      "Epoch: 029/050 training accuracy: 90.56%\n",
      "Epoch: 030/050 | Batch 000/105 | Cost: 0.2558\n",
      "Epoch: 030/050 | Batch 010/105 | Cost: 0.1287\n",
      "Epoch: 030/050 | Batch 020/105 | Cost: 0.1854\n",
      "Epoch: 030/050 | Batch 030/105 | Cost: 0.2453\n",
      "Epoch: 030/050 | Batch 040/105 | Cost: 0.1481\n",
      "Epoch: 030/050 | Batch 050/105 | Cost: 0.1736\n",
      "Epoch: 030/050 | Batch 060/105 | Cost: 0.1546\n",
      "Epoch: 030/050 | Batch 070/105 | Cost: 0.2378\n",
      "Epoch: 030/050 | Batch 080/105 | Cost: 0.2657\n",
      "Epoch: 030/050 | Batch 090/105 | Cost: 0.1329\n",
      "Epoch: 030/050 | Batch 100/105 | Cost: 0.1804\n",
      "Epoch: 030/050 training accuracy: 90.59%\n",
      "Epoch: 031/050 | Batch 000/105 | Cost: 0.2115\n",
      "Epoch: 031/050 | Batch 010/105 | Cost: 0.2126\n",
      "Epoch: 031/050 | Batch 020/105 | Cost: 0.1704\n",
      "Epoch: 031/050 | Batch 030/105 | Cost: 0.1979\n",
      "Epoch: 031/050 | Batch 040/105 | Cost: 0.2267\n",
      "Epoch: 031/050 | Batch 050/105 | Cost: 0.1903\n",
      "Epoch: 031/050 | Batch 060/105 | Cost: 0.2204\n",
      "Epoch: 031/050 | Batch 070/105 | Cost: 0.1867\n",
      "Epoch: 031/050 | Batch 080/105 | Cost: 0.2522\n",
      "Epoch: 031/050 | Batch 090/105 | Cost: 0.1514\n",
      "Epoch: 031/050 | Batch 100/105 | Cost: 0.2102\n",
      "Epoch: 031/050 training accuracy: 90.59%\n",
      "Epoch: 032/050 | Batch 000/105 | Cost: 0.1562\n",
      "Epoch: 032/050 | Batch 010/105 | Cost: 0.2210\n",
      "Epoch: 032/050 | Batch 020/105 | Cost: 0.1808\n",
      "Epoch: 032/050 | Batch 030/105 | Cost: 0.1757\n",
      "Epoch: 032/050 | Batch 040/105 | Cost: 0.2278\n",
      "Epoch: 032/050 | Batch 050/105 | Cost: 0.2350\n",
      "Epoch: 032/050 | Batch 060/105 | Cost: 0.1780\n",
      "Epoch: 032/050 | Batch 070/105 | Cost: 0.2877\n",
      "Epoch: 032/050 | Batch 080/105 | Cost: 0.1743\n",
      "Epoch: 032/050 | Batch 090/105 | Cost: 0.1765\n",
      "Epoch: 032/050 | Batch 100/105 | Cost: 0.1887\n",
      "Epoch: 032/050 training accuracy: 90.62%\n",
      "Epoch: 033/050 | Batch 000/105 | Cost: 0.1542\n",
      "Epoch: 033/050 | Batch 010/105 | Cost: 0.2372\n",
      "Epoch: 033/050 | Batch 020/105 | Cost: 0.2928\n",
      "Epoch: 033/050 | Batch 030/105 | Cost: 0.2398\n",
      "Epoch: 033/050 | Batch 040/105 | Cost: 0.1737\n",
      "Epoch: 033/050 | Batch 050/105 | Cost: 0.2210\n",
      "Epoch: 033/050 | Batch 060/105 | Cost: 0.1652\n",
      "Epoch: 033/050 | Batch 070/105 | Cost: 0.1402\n",
      "Epoch: 033/050 | Batch 080/105 | Cost: 0.2081\n",
      "Epoch: 033/050 | Batch 090/105 | Cost: 0.1584\n",
      "Epoch: 033/050 | Batch 100/105 | Cost: 0.2111\n",
      "Epoch: 033/050 training accuracy: 90.61%\n",
      "Epoch: 034/050 | Batch 000/105 | Cost: 0.2679\n",
      "Epoch: 034/050 | Batch 010/105 | Cost: 0.1904\n",
      "Epoch: 034/050 | Batch 020/105 | Cost: 0.2006\n",
      "Epoch: 034/050 | Batch 030/105 | Cost: 0.2337\n",
      "Epoch: 034/050 | Batch 040/105 | Cost: 0.1740\n",
      "Epoch: 034/050 | Batch 050/105 | Cost: 0.1915\n",
      "Epoch: 034/050 | Batch 060/105 | Cost: 0.2143\n",
      "Epoch: 034/050 | Batch 070/105 | Cost: 0.1376\n",
      "Epoch: 034/050 | Batch 080/105 | Cost: 0.2108\n",
      "Epoch: 034/050 | Batch 090/105 | Cost: 0.1707\n",
      "Epoch: 034/050 | Batch 100/105 | Cost: 0.1348\n",
      "Epoch: 034/050 training accuracy: 90.71%\n",
      "Epoch: 035/050 | Batch 000/105 | Cost: 0.2316\n",
      "Epoch: 035/050 | Batch 010/105 | Cost: 0.1448\n",
      "Epoch: 035/050 | Batch 020/105 | Cost: 0.2087\n",
      "Epoch: 035/050 | Batch 030/105 | Cost: 0.1462\n",
      "Epoch: 035/050 | Batch 040/105 | Cost: 0.1399\n",
      "Epoch: 035/050 | Batch 050/105 | Cost: 0.2179\n",
      "Epoch: 035/050 | Batch 060/105 | Cost: 0.2158\n",
      "Epoch: 035/050 | Batch 070/105 | Cost: 0.1871\n",
      "Epoch: 035/050 | Batch 080/105 | Cost: 0.1956\n",
      "Epoch: 035/050 | Batch 090/105 | Cost: 0.1509\n",
      "Epoch: 035/050 | Batch 100/105 | Cost: 0.1627\n",
      "Epoch: 035/050 training accuracy: 90.78%\n",
      "Epoch: 036/050 | Batch 000/105 | Cost: 0.2185\n",
      "Epoch: 036/050 | Batch 010/105 | Cost: 0.2021\n",
      "Epoch: 036/050 | Batch 020/105 | Cost: 0.2523\n",
      "Epoch: 036/050 | Batch 030/105 | Cost: 0.1706\n",
      "Epoch: 036/050 | Batch 040/105 | Cost: 0.2104\n",
      "Epoch: 036/050 | Batch 050/105 | Cost: 0.1702\n",
      "Epoch: 036/050 | Batch 060/105 | Cost: 0.1447\n",
      "Epoch: 036/050 | Batch 070/105 | Cost: 0.2138\n",
      "Epoch: 036/050 | Batch 080/105 | Cost: 0.1579\n",
      "Epoch: 036/050 | Batch 090/105 | Cost: 0.1562\n",
      "Epoch: 036/050 | Batch 100/105 | Cost: 0.3083\n",
      "Epoch: 036/050 training accuracy: 90.76%\n",
      "Epoch: 037/050 | Batch 000/105 | Cost: 0.1615\n",
      "Epoch: 037/050 | Batch 010/105 | Cost: 0.1821\n",
      "Epoch: 037/050 | Batch 020/105 | Cost: 0.1700\n",
      "Epoch: 037/050 | Batch 030/105 | Cost: 0.2483\n",
      "Epoch: 037/050 | Batch 040/105 | Cost: 0.1786\n",
      "Epoch: 037/050 | Batch 050/105 | Cost: 0.1315\n",
      "Epoch: 037/050 | Batch 060/105 | Cost: 0.1943\n",
      "Epoch: 037/050 | Batch 070/105 | Cost: 0.1665\n",
      "Epoch: 037/050 | Batch 080/105 | Cost: 0.1146\n",
      "Epoch: 037/050 | Batch 090/105 | Cost: 0.1767\n",
      "Epoch: 037/050 | Batch 100/105 | Cost: 0.2225\n",
      "Epoch: 037/050 training accuracy: 90.80%\n",
      "Epoch: 038/050 | Batch 000/105 | Cost: 0.2258\n",
      "Epoch: 038/050 | Batch 010/105 | Cost: 0.1888\n",
      "Epoch: 038/050 | Batch 020/105 | Cost: 0.1710\n",
      "Epoch: 038/050 | Batch 030/105 | Cost: 0.1245\n",
      "Epoch: 038/050 | Batch 040/105 | Cost: 0.2151\n",
      "Epoch: 038/050 | Batch 050/105 | Cost: 0.2022\n",
      "Epoch: 038/050 | Batch 060/105 | Cost: 0.1304\n",
      "Epoch: 038/050 | Batch 070/105 | Cost: 0.2160\n",
      "Epoch: 038/050 | Batch 080/105 | Cost: 0.2391\n",
      "Epoch: 038/050 | Batch 090/105 | Cost: 0.1620\n",
      "Epoch: 038/050 | Batch 100/105 | Cost: 0.1697\n",
      "Epoch: 038/050 training accuracy: 90.82%\n",
      "Epoch: 039/050 | Batch 000/105 | Cost: 0.2060\n",
      "Epoch: 039/050 | Batch 010/105 | Cost: 0.2121\n",
      "Epoch: 039/050 | Batch 020/105 | Cost: 0.1823\n",
      "Epoch: 039/050 | Batch 030/105 | Cost: 0.1591\n",
      "Epoch: 039/050 | Batch 040/105 | Cost: 0.2249\n",
      "Epoch: 039/050 | Batch 050/105 | Cost: 0.1902\n",
      "Epoch: 039/050 | Batch 060/105 | Cost: 0.2264\n",
      "Epoch: 039/050 | Batch 070/105 | Cost: 0.2491\n",
      "Epoch: 039/050 | Batch 080/105 | Cost: 0.0974\n",
      "Epoch: 039/050 | Batch 090/105 | Cost: 0.2251\n",
      "Epoch: 039/050 | Batch 100/105 | Cost: 0.1536\n",
      "Epoch: 039/050 training accuracy: 90.82%\n",
      "Epoch: 040/050 | Batch 000/105 | Cost: 0.1703\n",
      "Epoch: 040/050 | Batch 010/105 | Cost: 0.2074\n",
      "Epoch: 040/050 | Batch 020/105 | Cost: 0.1070\n",
      "Epoch: 040/050 | Batch 030/105 | Cost: 0.2116\n",
      "Epoch: 040/050 | Batch 040/105 | Cost: 0.1929\n",
      "Epoch: 040/050 | Batch 050/105 | Cost: 0.1926\n",
      "Epoch: 040/050 | Batch 060/105 | Cost: 0.1851\n",
      "Epoch: 040/050 | Batch 070/105 | Cost: 0.2234\n",
      "Epoch: 040/050 | Batch 080/105 | Cost: 0.1449\n",
      "Epoch: 040/050 | Batch 090/105 | Cost: 0.1546\n",
      "Epoch: 040/050 | Batch 100/105 | Cost: 0.1680\n",
      "Epoch: 040/050 training accuracy: 90.83%\n",
      "Epoch: 041/050 | Batch 000/105 | Cost: 0.2046\n",
      "Epoch: 041/050 | Batch 010/105 | Cost: 0.1579\n",
      "Epoch: 041/050 | Batch 020/105 | Cost: 0.1842\n",
      "Epoch: 041/050 | Batch 030/105 | Cost: 0.2346\n",
      "Epoch: 041/050 | Batch 040/105 | Cost: 0.1419\n",
      "Epoch: 041/050 | Batch 050/105 | Cost: 0.1214\n",
      "Epoch: 041/050 | Batch 060/105 | Cost: 0.2098\n",
      "Epoch: 041/050 | Batch 070/105 | Cost: 0.1454\n",
      "Epoch: 041/050 | Batch 080/105 | Cost: 0.2164\n",
      "Epoch: 041/050 | Batch 090/105 | Cost: 0.1999\n",
      "Epoch: 041/050 | Batch 100/105 | Cost: 0.1798\n",
      "Epoch: 041/050 training accuracy: 90.83%\n",
      "Epoch: 042/050 | Batch 000/105 | Cost: 0.2759\n",
      "Epoch: 042/050 | Batch 010/105 | Cost: 0.1435\n",
      "Epoch: 042/050 | Batch 020/105 | Cost: 0.1866\n",
      "Epoch: 042/050 | Batch 030/105 | Cost: 0.1633\n",
      "Epoch: 042/050 | Batch 040/105 | Cost: 0.1699\n",
      "Epoch: 042/050 | Batch 050/105 | Cost: 0.1834\n",
      "Epoch: 042/050 | Batch 060/105 | Cost: 0.2168\n",
      "Epoch: 042/050 | Batch 070/105 | Cost: 0.1945\n",
      "Epoch: 042/050 | Batch 080/105 | Cost: 0.1981\n",
      "Epoch: 042/050 | Batch 090/105 | Cost: 0.2099\n",
      "Epoch: 042/050 | Batch 100/105 | Cost: 0.1978\n",
      "Epoch: 042/050 training accuracy: 90.86%\n",
      "Epoch: 043/050 | Batch 000/105 | Cost: 0.1692\n",
      "Epoch: 043/050 | Batch 010/105 | Cost: 0.1866\n",
      "Epoch: 043/050 | Batch 020/105 | Cost: 0.2228\n",
      "Epoch: 043/050 | Batch 030/105 | Cost: 0.1463\n",
      "Epoch: 043/050 | Batch 040/105 | Cost: 0.1822\n",
      "Epoch: 043/050 | Batch 050/105 | Cost: 0.2430\n",
      "Epoch: 043/050 | Batch 060/105 | Cost: 0.2091\n",
      "Epoch: 043/050 | Batch 070/105 | Cost: 0.1342\n",
      "Epoch: 043/050 | Batch 080/105 | Cost: 0.1240\n",
      "Epoch: 043/050 | Batch 090/105 | Cost: 0.2727\n",
      "Epoch: 043/050 | Batch 100/105 | Cost: 0.1896\n",
      "Epoch: 043/050 training accuracy: 90.92%\n",
      "Epoch: 044/050 | Batch 000/105 | Cost: 0.1616\n",
      "Epoch: 044/050 | Batch 010/105 | Cost: 0.0763\n",
      "Epoch: 044/050 | Batch 020/105 | Cost: 0.1422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 044/050 | Batch 030/105 | Cost: 0.2344\n",
      "Epoch: 044/050 | Batch 040/105 | Cost: 0.1549\n",
      "Epoch: 044/050 | Batch 050/105 | Cost: 0.2250\n",
      "Epoch: 044/050 | Batch 060/105 | Cost: 0.0711\n",
      "Epoch: 044/050 | Batch 070/105 | Cost: 0.1920\n",
      "Epoch: 044/050 | Batch 080/105 | Cost: 0.2117\n",
      "Epoch: 044/050 | Batch 090/105 | Cost: 0.2056\n",
      "Epoch: 044/050 | Batch 100/105 | Cost: 0.1605\n",
      "Epoch: 044/050 training accuracy: 90.94%\n",
      "Epoch: 045/050 | Batch 000/105 | Cost: 0.1731\n",
      "Epoch: 045/050 | Batch 010/105 | Cost: 0.1230\n",
      "Epoch: 045/050 | Batch 020/105 | Cost: 0.2082\n",
      "Epoch: 045/050 | Batch 030/105 | Cost: 0.1977\n",
      "Epoch: 045/050 | Batch 040/105 | Cost: 0.1137\n",
      "Epoch: 045/050 | Batch 050/105 | Cost: 0.1359\n",
      "Epoch: 045/050 | Batch 060/105 | Cost: 0.1544\n",
      "Epoch: 045/050 | Batch 070/105 | Cost: 0.1347\n",
      "Epoch: 045/050 | Batch 080/105 | Cost: 0.1961\n",
      "Epoch: 045/050 | Batch 090/105 | Cost: 0.1926\n",
      "Epoch: 045/050 | Batch 100/105 | Cost: 0.1685\n",
      "Epoch: 045/050 training accuracy: 90.93%\n",
      "Epoch: 046/050 | Batch 000/105 | Cost: 0.1739\n",
      "Epoch: 046/050 | Batch 010/105 | Cost: 0.1970\n",
      "Epoch: 046/050 | Batch 020/105 | Cost: 0.1129\n",
      "Epoch: 046/050 | Batch 030/105 | Cost: 0.2180\n",
      "Epoch: 046/050 | Batch 040/105 | Cost: 0.1836\n",
      "Epoch: 046/050 | Batch 050/105 | Cost: 0.1568\n",
      "Epoch: 046/050 | Batch 060/105 | Cost: 0.1082\n",
      "Epoch: 046/050 | Batch 070/105 | Cost: 0.1853\n",
      "Epoch: 046/050 | Batch 080/105 | Cost: 0.1676\n",
      "Epoch: 046/050 | Batch 090/105 | Cost: 0.1213\n",
      "Epoch: 046/050 | Batch 100/105 | Cost: 0.1381\n",
      "Epoch: 046/050 training accuracy: 90.94%\n",
      "Epoch: 047/050 | Batch 000/105 | Cost: 0.2062\n",
      "Epoch: 047/050 | Batch 010/105 | Cost: 0.1452\n",
      "Epoch: 047/050 | Batch 020/105 | Cost: 0.1877\n",
      "Epoch: 047/050 | Batch 030/105 | Cost: 0.1931\n",
      "Epoch: 047/050 | Batch 040/105 | Cost: 0.1510\n",
      "Epoch: 047/050 | Batch 050/105 | Cost: 0.1972\n",
      "Epoch: 047/050 | Batch 060/105 | Cost: 0.2062\n",
      "Epoch: 047/050 | Batch 070/105 | Cost: 0.1908\n",
      "Epoch: 047/050 | Batch 080/105 | Cost: 0.1823\n",
      "Epoch: 047/050 | Batch 090/105 | Cost: 0.2353\n",
      "Epoch: 047/050 | Batch 100/105 | Cost: 0.1266\n",
      "Epoch: 047/050 training accuracy: 90.98%\n",
      "Epoch: 048/050 | Batch 000/105 | Cost: 0.1143\n",
      "Epoch: 048/050 | Batch 010/105 | Cost: 0.1333\n",
      "Epoch: 048/050 | Batch 020/105 | Cost: 0.1380\n",
      "Epoch: 048/050 | Batch 030/105 | Cost: 0.2048\n",
      "Epoch: 048/050 | Batch 040/105 | Cost: 0.1871\n",
      "Epoch: 048/050 | Batch 050/105 | Cost: 0.2020\n",
      "Epoch: 048/050 | Batch 060/105 | Cost: 0.1409\n",
      "Epoch: 048/050 | Batch 070/105 | Cost: 0.1347\n",
      "Epoch: 048/050 | Batch 080/105 | Cost: 0.1627\n",
      "Epoch: 048/050 | Batch 090/105 | Cost: 0.1919\n",
      "Epoch: 048/050 | Batch 100/105 | Cost: 0.1774\n",
      "Epoch: 048/050 training accuracy: 90.98%\n",
      "Epoch: 049/050 | Batch 000/105 | Cost: 0.0875\n",
      "Epoch: 049/050 | Batch 010/105 | Cost: 0.1649\n",
      "Epoch: 049/050 | Batch 020/105 | Cost: 0.1659\n",
      "Epoch: 049/050 | Batch 030/105 | Cost: 0.1147\n",
      "Epoch: 049/050 | Batch 040/105 | Cost: 0.0933\n",
      "Epoch: 049/050 | Batch 050/105 | Cost: 0.1558\n",
      "Epoch: 049/050 | Batch 060/105 | Cost: 0.1761\n",
      "Epoch: 049/050 | Batch 070/105 | Cost: 0.2067\n",
      "Epoch: 049/050 | Batch 080/105 | Cost: 0.2025\n",
      "Epoch: 049/050 | Batch 090/105 | Cost: 0.2658\n",
      "Epoch: 049/050 | Batch 100/105 | Cost: 0.1739\n",
      "Epoch: 049/050 training accuracy: 91.00%\n",
      "Epoch: 050/050 | Batch 000/105 | Cost: 0.1838\n",
      "Epoch: 050/050 | Batch 010/105 | Cost: 0.1762\n",
      "Epoch: 050/050 | Batch 020/105 | Cost: 0.1315\n",
      "Epoch: 050/050 | Batch 030/105 | Cost: 0.1759\n",
      "Epoch: 050/050 | Batch 040/105 | Cost: 0.1003\n",
      "Epoch: 050/050 | Batch 050/105 | Cost: 0.1291\n",
      "Epoch: 050/050 | Batch 060/105 | Cost: 0.1327\n",
      "Epoch: 050/050 | Batch 070/105 | Cost: 0.1515\n",
      "Epoch: 050/050 | Batch 080/105 | Cost: 0.1479\n",
      "Epoch: 050/050 | Batch 090/105 | Cost: 0.2363\n",
      "Epoch: 050/050 | Batch 100/105 | Cost: 0.1464\n",
      "Epoch: 050/050 training accuracy: 90.96%\n",
      "Test Metrics:\n",
      "AUC_val: 0.8751\n",
      "ACC_val: 0.8779\n",
      "MCC_val: 0.4373\n",
      "Sn_val: 0.6792\n",
      "Sp_val: 0.3634\n",
      "Time elapsed: 1.99 min\n",
      "Total Training Time: 1.99 min\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "i = 1\n",
    "for train_dataset_indx, val_dataset_indx in kf.split(X=train_dataset_unsplit):\n",
    "    \n",
    "    # print(len(train_dataset_unsplit))\n",
    "    print(\"iteration \", i)\n",
    "    print(\"train_dataset_indx\", \" having :\" , len(train_dataset_indx))\n",
    "    print(\"val_dataset_indx\", \" having :\" , len(val_dataset_indx))\n",
    "    print(\"-------------------------\")\n",
    "    i += 1\n",
    "    \n",
    "    train_dataset = Subset(train_dataset_unsplit, train_dataset_indx)\n",
    "    val_dataset = Subset(train_dataset_unsplit, val_dataset_indx)\n",
    "    \n",
    "    # train_dataset = train_dataset_unsplit[train_dataset_indx,:,:]\n",
    "    # val_dataset = train_dataset_unsplit[val_dataset_indx,:,:]\n",
    "    \n",
    "    # for i in range(len(train_dataset_unsplit)):\n",
    "    #     features, label = train_dataset_unsplit[i]\n",
    "    #     print(f\"Sample {i}: Features - {features}, Label - {label}\")\n",
    "    \n",
    "################################################ \n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(dataset=val_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "    \n",
    "    test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "    # Checking the dataset \n",
    "    for images, labels in train_loader: \n",
    "        print('Image batch dimensions:', images.shape)\n",
    "        print('Image label dimensions:', labels.shape)\n",
    "        break\n",
    "        \n",
    "################################################ \n",
    "predicted_probabilities = []  \n",
    "true_labels = []  \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(features)\n",
    "        cost = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if batch_idx % 10 == 0: \n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model = model.eval() \n",
    "    \n",
    "    with torch.set_grad_enabled(False): \n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader)))\n",
    "        \n",
    "        for batch_idx, (features, targets) in enumerate(val_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            logits = model(features)\n",
    "            probabilities = torch.sigmoid(logits)[:,1] \n",
    "            \n",
    "            predicted_probabilities.extend(probabilities.tolist()) \n",
    "            true_labels.extend(targets.tolist()) \n",
    "    \n",
    "\n",
    "\n",
    "AUC_val = roc_auc_score(true_labels, predicted_probabilities)\n",
    "predicted_probabilities = np.array(predicted_probabilities)\n",
    "ACC_val = accuracy_score(true_labels, (predicted_probabilities > 0.5).astype(int)) \n",
    "MCC_val = matthews_corrcoef(true_labels, (predicted_probabilities > 0.5).astype(int))\n",
    "Sn_val = recall_score(true_labels, (predicted_probabilities > 0.5).astype(int))\n",
    "Sp_val = precision_score(true_labels, (predicted_probabilities > 0.5).astype(int))\n",
    "    \n",
    "print(\"Test Metrics:\")\n",
    "print(\"AUC_val: {:.4f}\".format(AUC_val))\n",
    "print(\"ACC_val: {:.4f}\".format(ACC_val))\n",
    "print(\"MCC_val: {:.4f}\".format(MCC_val))\n",
    "print(\"Sn_val: {:.4f}\".format(Sn_val))\n",
    "print(\"Sp_val: {:.4f}\".format(Sp_val))\n",
    "print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc45f564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "AUC_test: 0.9471\n",
      "ACC_test: 0.9176\n",
      "MCC_test: 0.5864\n",
      "Sn_test: 0.7610\n",
      "Sp_test: 0.5197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "\n",
    "model = model.eval()\n",
    "predicted_probabilities = []  \n",
    "true_labels = [] \n",
    "\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            logits = model(features)\n",
    "            probabilities = torch.sigmoid(logits)[:,1] \n",
    "            \n",
    "            predicted_probabilities.extend(probabilities.tolist())\n",
    "            true_labels.extend(targets.tolist()) \n",
    "    \n",
    "\n",
    "    AUC_test = roc_auc_score(true_labels, predicted_probabilities)\n",
    "    predicted_probabilities = np.array(predicted_probabilities)\n",
    "    ACC_test = accuracy_score(true_labels, (predicted_probabilities > 0.5).astype(int)) \n",
    "    MCC_test = matthews_corrcoef(true_labels, (predicted_probabilities > 0.5).astype(int))\n",
    "    Sn_test = recall_score(true_labels, (predicted_probabilities > 0.5).astype(int))\n",
    "    Sp_test = precision_score(true_labels, (predicted_probabilities > 0.5).astype(int))\n",
    "    \n",
    "    print(\"Test Metrics:\")\n",
    "    print(\"AUC_test: {:.4f}\".format(AUC_test))\n",
    "    print(\"ACC_test: {:.4f}\".format(ACC_test))\n",
    "    print(\"MCC_test: {:.4f}\".format(MCC_test))\n",
    "    print(\"Sn_test: {:.4f}\".format(Sn_test))\n",
    "    print(\"Sp_test: {:.4f}\".format(Sp_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1261d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "torch.save(model.state_dict(), 'ResNet_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28795112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2d2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f96ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
